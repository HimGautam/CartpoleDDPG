{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(physical_devices)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfb28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from collections import deque\n",
    "import gym\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "input_size = len(obs)\n",
    "output_size = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.005\n",
    "gamma = 0.99\n",
    "l2_decay = 1e-2\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "memory_size = 1e6\n",
    "minibatch_size = 128\n",
    "\n",
    "num_episodes = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "        \n",
    "        self.actor.compile(optimizer = Adam(lr = actor_lr))\n",
    "        self.critic.compile(optimizer = Adam(lr = critic_lr))\n",
    "        \n",
    "        self.stable_actor = self.actor\n",
    "        self.stable_critic = self.critic\n",
    "        \n",
    "        self.memory = deque()\n",
    "        \n",
    "        \n",
    "    def build_actor(self):\n",
    "        inputs = Input(shape = input_size)\n",
    "        x = Dense(256, activation = 'relu')(inputs)\n",
    "        x = Dense(64, activation = 'relu')(x)\n",
    "        x = Dense(output_size, activation = \"linear\")(x)\n",
    "        model = Model(inputs = inputs, outputs = x)\n",
    "        return model\n",
    "    \n",
    "    def build_critic(self):\n",
    "        inputs = Input(shape = input_size + output_size)\n",
    "        x = Dense(256, activation = 'relu')(inputs)\n",
    "        x = Dense(64, activation = 'relu')(x)\n",
    "        x = Dense(1, activation = 'linear')(x)\n",
    "        model = Model(inputs = inputs, outputs = x)\n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        if len(self.memory)>memory_size:\n",
    "            random.shuffle(self.memory)\n",
    "            self.memory.popleft()\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    @tf.function \n",
    "    def act(self, state, test = False):\n",
    "        act_value = self.actor(state)\n",
    "        if not test:\n",
    "            act_value += tf.random.normal(shape = act_value.shape, mean=0.0, stddev=0.1)\n",
    "        act_value = tf.nn.softmax(act_value)\n",
    "        return act_value\n",
    "      \n",
    "    def learn(self):\n",
    "        random.shuffle(self.memory)\n",
    "        minibatch = random.sample(self.memory, minibatch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        \n",
    "        actions = np.squeeze(actions)\n",
    "        next_actions = self.stable_actor(next_states)\n",
    "        next_actions = tf.nn.softmax(next_actions)\n",
    "        \n",
    "        target_q_values = rewards + gamma * self.stable_critic(tf.concat([next_states, next_actions], axis = 1))\n",
    "        target_q_values = target_q_values.numpy()\n",
    "        \n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                target_q_values[i] = rewards[i]\n",
    "                \n",
    "        with tf.GradientTape() as tape:        \n",
    "            pred_q_values = self.critic(np.concatenate([states, actions], axis = 1))\n",
    "            critic_loss = tf.reduce_mean((target_q_values - pred_q_values)**2)\n",
    "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_weights)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_weights))\n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(states)\n",
    "            actions = tf.nn.softmax(actions)\n",
    "            actor_loss = -self.critic(tf.concat([states, actions], axis = 1))\n",
    "            actor_loss = tf.reduce_mean(actor_loss)\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_weights)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_weights))\n",
    "        self.update_target_weights()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def update_target_weights(self):\n",
    "        weights = []\n",
    "        target_weights = self.stable_actor.get_weights()\n",
    "        for i, weight in enumerate(self.actor.get_weights()):\n",
    "            weights.append(weight * tau + target_weights[i] * (1-tau))\n",
    "        self.stable_actor.set_weights(weights)\n",
    "        \n",
    "        weights = []\n",
    "        target_weights = self.stable_critic.get_weights()\n",
    "        for i, weight in enumerate(self.critic.get_weights()):\n",
    "            weights.append(weight * tau + target_weights[i] * (1-tau))\n",
    "        self.stable_critic.set_weights(weights)\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.actor.load_weights(name + \"actor.hdf5\")\n",
    "        self.critic.load_weights(name + \"critic.hdf5\")\n",
    "        self.stable_actor.load_weights(name + \"actor.hdf5\")\n",
    "        self.stable_critic.load_weights(name + \"critic.hdf5\")\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.actor.save_weights(name + \"actor.hdf5\")\n",
    "        self.critic.save_weights(name + \"critic.hdf5\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767e128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training \n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "for ep in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action =  agent.act(np.expand_dims(state, axis = 0))\n",
    "        next_state, reward, done, _ =  env.step(np.squeeze(np.argmax(action, axis =1)))\n",
    "        score += reward\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        if len(agent.memory) > 4* minibatch_size:\n",
    "            agent.learn()\n",
    "        \n",
    "        state = next_state\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        agent.save(\"DDPG_\")\n",
    "    print(f\"Episode: {ep}, Score: {score}, Avg Score: {avg_score}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ad6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"DDPG_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac527eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "done = False\n",
    "state = env.reset()\n",
    "score=0\n",
    "t = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = agent.act(np.expand_dims(state, axis=0), test=False)\n",
    "    next_state, reward, done, _ = env.step(np.squeeze(np.argmax(action, axis = 1)))\n",
    "    score+= reward\n",
    "    state = next_state\n",
    "    t += 1\n",
    "    \n",
    "print(f\"Episode ended in {t} steps with score of {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85468f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
